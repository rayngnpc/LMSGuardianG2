    print('Exception:', e)
"
python3 -c "
import psycopg2
import os
from dotenv import load_dotenv

load_dotenv()

try:
    # Connect to database
    conn = psycopg2.connect(
        host=os.getenv('DATABASE_HOST', 'localhost'),
        database=os.getenv('DATABASE_NAME', 'lms_security_db'),
        user=os.getenv('DATABASE_USER', 'lms_user'),
        password=os.getenv('DATABASE_PASSWORD', 'lms_password123')
    )
    
    cur = conn.cursor()
    
    print('üîç ANALYZING DATABASE FOR PROBLEMATIC URLS...')
    
    # Count total records
    cur.execute('SELECT COUNT(*) FROM scraped_contents')
    total = cur.fetchone()[0]
    print(f'üìä Total records in database: {total}')
    
    # Look for specific problematic patterns from the image
    problematic_queries = [
        ('Empty HTTPS', \"SELECT * FROM scraped_contents WHERE url_link = 'https://'\"),
        ('Copilot Microsoft', \"SELECT * FROM scraped_contents WHERE url_link LIKE '%copilot.microsoft.com%'\"),
        ('GNU PSPP with parentheses', \"SELECT * FROM scraped_contents WHERE url_link LIKE '%gnu.org/software/pspp%' AND (url_link LIKE '%(%' OR url_link LIKE '%)%')\"),
        ('RMIT PDF encoded', \"SELECT * FROM scraped_contents WHERE url_link LIKE '%dlsweb.rmit.edu.au%' AND url_link LIKE '%PG%20lit%20Review%'\")
    ]
    
    for desc, query in problematic_queries:
        print(f'\n--- {desc} ---')
        cur.execute(query)
        results = cur.fetchall()
        
        if results:
            print(f'‚úÖ Found {len(results)} matches:')
            for row in results[:3]:  # Show first 3
                print(f'   Module {row[1]}, Session {row[2]}')
                print(f'   URL: {row[3]}')
                print(f'   Title: {row[4][:50] if row[4] else \"N/A\"}...')
                print(f'   Risk: {row[8] if len(row) > 8 else \"N/A\"}')
        else:
            print('‚ùå No matches found')
    
    # Check for general malformed URLs
    print('\nüîß CHECKING FOR MALFORMED URL PATTERNS...')
    
    malformed_queries = [
        ('URLs ending with just protocol', \"SELECT COUNT(*) FROM scraped_contents WHERE url_link IN ('https://', 'http://')\"),
        ('URLs with parentheses', \"SELECT COUNT(*) FROM scraped_contents WHERE url_link LIKE '%(%' OR url_link LIKE '%)%'\"),
        ('URLs with excessive encoding', \"SELECT COUNT(*) FROM scraped_contents WHERE url_link LIKE '%20%20%20%'\")
    ]
    
    for desc, query in malformed_queries:
        cur.execute(query)
        count = cur.fetchone()[0]
        print(f'   {desc}: {count} instances')
    
    conn.close()
    
except Exception as e:
    print(f'‚ùå Database error: {e}')
    import traceback
    traceback.print_exc()
"
python3 -c "
import psycopg2
import os

try:
    # Connect to database with correct credentials
    conn = psycopg2.connect(
        host='localhost',
        database='lmsguardian',
        user='admin',
        password='group2'
    )
    
    cur = conn.cursor()
    
    print('üîç ANALYZING DATABASE FOR PROBLEMATIC URLS FROM REPORT...')
    
    # Count total records
    cur.execute('SELECT COUNT(*) FROM scraped_contents')
    total = cur.fetchone()[0]
    print(f'üìä Total records in database: {total}')
    
    # Look for specific problematic patterns from the image
    problematic_queries = [
        ('Empty HTTPS Protocol', \"SELECT * FROM scraped_contents WHERE url_link = 'https://'\"),
        ('Microsoft Copilot', \"SELECT * FROM scraped_contents WHERE url_link LIKE '%copilot.microsoft.com%'\"),
        ('GNU PSPP with parentheses', \"SELECT * FROM scraped_contents WHERE url_link LIKE '%gnu.org/software/pspp%' AND (url_link LIKE '%(%' OR url_link LIKE '%)%')\"),
        ('RMIT PDF with encoding', \"SELECT * FROM scraped_contents WHERE url_link LIKE '%dlsweb.rmit.edu.au%' AND url_link LIKE '%PG%20lit%20Review%'\")
    ]
    
    total_problematic = 0
    
    for desc, query in problematic_queries:
        print(f'\n--- {desc} ---')
        cur.execute(query)
        results = cur.fetchall()
        
        if results:
            print(f'‚úÖ Found {len(results)} matches:')
            total_problematic += len(results)
            for row in results[:2]:  # Show first 2
                print(f'   Module {row[1]}, Session {row[2]}')
                print(f'   URL: {row[3]}')
                print(f'   Title: {row[4][:50] if row[4] else \"N/A\"}...')
                print(f'   Scraped: {row[6] if len(row) > 6 else \"N/A\"}')
                if len(row) > 8:
                    print(f'   Risk: {row[8] if row[8] else \"None\"}')
        else:
            print('‚ùå No matches found')
    
    # Check for general malformed URL patterns
    print(f'\nüîß CHECKING FOR MALFORMED URL PATTERNS...')
    
    malformed_queries = [
        ('Empty protocol URLs', \"SELECT COUNT(*) FROM scraped_contents WHERE url_link IN ('https://', 'http://')\"),
        ('URLs with parentheses', \"SELECT COUNT(*) FROM scraped_contents WHERE url_link LIKE '%(%' OR url_link LIKE '%)%'\"),
        ('URLs with excessive encoding', \"SELECT COUNT(*) FROM scraped_contents WHERE url_link LIKE '%20%20%20%'\"),
        ('Malformed HTTPS URLs', \"SELECT COUNT(*) FROM scraped_contents WHERE url_link LIKE 'https://%' AND LENGTH(url_link) < 15\")
    ]
    
    for desc, query in malformed_queries:
        cur.execute(query)
        count = cur.fetchone()[0]
        if count > 0:
            print(f'   ‚ö†Ô∏è  {desc}: {count} instances')
        else:
            print(f'   ‚úÖ {desc}: {count} instances')
    
    print(f'\nüìä SUMMARY:')
    print(f'   Total problematic URLs found: {total_problematic}')
    print(f'   These match the broken links in your report image.')
    
    conn.close()
    
except Exception as e:
    print(f'‚ùå Database error: {e}')
    import traceback
    traceback.print_exc()
"
source venv/bin/activate
cd /home/administrator/Test-Chau-LMS/LMSTest && python3 -c "
from scraper.reportgenerator.report import fetch_all_links_by_module
import sys

print('ÔøΩÔøΩ TESTING REPORT GENERATION TO ANALYZE PROBLEMATIC URLS...')

try:
    # Fetch the data that would be used in report generation
    modules_data = fetch_all_links_by_module()
    
    print(f'\nüìä MODULES DATA RETRIEVED:')
    total_urls = 0
    
    for module_id, links in modules_data.items():
        print(f'\n--- Module {module_id} ---')
        print(f'   Total URLs: {len(links)}')
        total_urls += len(links)
        
        # Look for problematic URLs specifically
        problematic_urls = []
        
        for link in links:
            url = link.get('url_link', '')
            
            # Check for the problematic patterns from the image
            if url == 'https://':
                problematic_urls.append(('EMPTY_HTTPS', url))
            elif 'copilot.microsoft.com' in url:
                problematic_urls.append(('COPILOT_MS', url))
            elif 'gnu.org/software/pspp' in url and ('(' in url or ')' in url):
                problematic_urls.append(('GNU_PSPP_PARENTHESES', url))
            elif 'dlsweb.rmit.edu.au' in url and 'PG%20lit%20Review' in url:
                problematic_urls.append(('RMIT_PDF_ENCODED', url))
            elif len(url) < 10 or url.count('/') < 2:
                problematic_urls.append(('MALFORMED_URL', url))
        
        if problematic_urls:
            print(f'   üö® PROBLEMATIC URLs found: {len(problematic_urls)}')
            for issue_type, url in problematic_urls[:3]:  # Show first 3
                print(f'      {issue_type}: {url}')
        else:
            print(f'   ‚úÖ No problematic URLs found')
        
        # Show a sample of normal URLs
        print(f'   üìã Sample URLs:')
        for link in links[:3]:
            url = link.get('url_link', '')
            title = link.get('page_title', 'N/A')
            print(f'      {url[:60]}... | {title[:30]}...')
    
    print(f'\nüìä TOTAL ANALYSIS:')
    print(f'   Total modules: {len(modules_data)}')
    print(f'   Total URLs across all modules: {total_urls}')
    print(f'   This data would be used to generate the security reports')

except Exception as e:
    print(f'‚ùå Error: {e}')
    import traceback
    traceback.print_exc()
"
curl -s http://127.0.0.1:8000/scrapedcontents/ | python3 -m json.tool | head -50
netstat -tlnp | grep 8000
ss -tlnp | grep 8000
python3 -c "
import psycopg2

try:
    # Connect with correct credentials
    conn = psycopg2.connect(
        host='localhost',
        database='lmsguardian', 
        user='admin',
        password='group2'
    )
    
    cur = conn.cursor()
    
    print('üîç INVESTIGATING STALE/CACHED DATA IN DATABASE...')
    
    # Check what's actually in Module 1
    print('\n--- MODULE 1 ANALYSIS ---')
    cur.execute('''
        SELECT session_id, url_link, page_title, scraped_at 
        FROM scraped_contents 
        WHERE module_id = 1 
        ORDER BY session_id DESC, scraped_at DESC
    ''')
    
    module1_data = cur.fetchall()
    print(f'üìä Total entries for Module 1: {len(module1_data)}')
    
    # Group by session to see what sessions exist
    sessions = {}
    for row in module1_data:
        session_id = row[0]
        if session_id not in sessions:
            sessions[session_id] = []
        sessions[session_id].append(row)
    
    print(f'üìã Sessions found for Module 1: {list(sessions.keys())}')
    
    # Analyze each session
    for session_id in sorted(sessions.keys(), reverse=True):
        session_data = sessions[session_id]
        print(f'\nüïê SESSION {session_id} ({len(session_data)} URLs):')
        
        # Look for suspicious URLs
        suspicious = []
        legitimate = []
        
        for row in session_data:
            url = row[1]
            title = row[2] or 'No title'
            scraped_at = row[3]
            
            # Check for problematic patterns
            if url == 'https://':
                suspicious.append(('EMPTY_HTTPS', url, title, scraped_at))
            elif 'copilot.microsoft.com' in url:
                suspicious.append(('COPILOT_MS', url, title, scraped_at))
            elif 'gnu.org/software/pspp' in url and ('(' in url or ')' in url):
                suspicious.append(('GNU_PSPP_MALFORMED', url, title, scraped_at))
            elif len(url) < 15 and url.startswith('http'):
                suspicious.append(('MALFORMED_SHORT', url, title, scraped_at))
            else:
                legitimate.append((url, title, scraped_at))
        
        if suspicious:
            print(f'   üö® SUSPICIOUS URLs ({len(suspicious)}):')
            for issue_type, url, title, scraped_at in suspicious:
                print(f'      {issue_type}: {url}')
                print(f'         Title: {title[:50]}...')
                print(f'         Scraped: {scraped_at}')
                print()
        
        print(f'   ‚úÖ Legitimate URLs: {len(legitimate)}')
        if legitimate:
            print(f'      Sample: {legitimate[0][0][:60]}...')
    
    # Check if there are multiple sessions and when they were created
    print(f'\nüïê SESSION TIMELINE ANALYSIS:')
    cur.execute('''
        SELECT session_id, MIN(scraped_at) as first_scrape, MAX(scraped_at) as last_scrape, COUNT(*) as url_count
        FROM scraped_contents 
        WHERE module_id = 1 
        GROUP BY session_id 
        ORDER BY session_id DESC
    ''')
    
    session_timeline = cur.fetchall()
    for row in session_timeline:
        session_id, first_scrape, last_scrape, url_count = row
        print(f'   Session {session_id}: {url_count} URLs, {first_scrape} to {last_scrape}')
    
    conn.close()
    
except Exception as e:
    print(f'‚ùå Error: {e}')
    import traceback
    traceback.print_exc()
"
python3 -c "
import psycopg2

try:
    # Connect with correct credentials
    conn = psycopg2.connect(
        host='localhost',
        database='lmsguardian', 
        user='admin',
        password='group2'
    )
    
    cur = conn.cursor()
    
    print('üîç CHECKING DATABASE SCHEMA FIRST...')
    
    # Check the actual column names
    cur.execute('''
        SELECT column_name 
        FROM information_schema.columns 
        WHERE table_name = 'scraped_contents'
        ORDER BY ordinal_position
    ''')
    
    columns = [row[0] for row in cur.fetchall()]
    print(f'üìã Available columns: {columns}')
    
    # Now query with correct column names
    print(f'\n--- MODULE 1 ANALYSIS (with correct columns) ---')
    cur.execute('''
        SELECT session_id, url_link, title, scraped_at 
        FROM scraped_contents 
        WHERE module_id = 1 
        ORDER BY session_id DESC, scraped_at DESC
    ''')
    
    module1_data = cur.fetchall()
    print(f'üìä Total entries for Module 1: {len(module1_data)}')
    
    # Group by session
    sessions = {}
    for row in module1_data:
        session_id = row[0]
        if session_id not in sessions:
            sessions[session_id] = []
        sessions[session_id].append(row)
    
    print(f'üìã Sessions found for Module 1: {sorted(sessions.keys(), reverse=True)}')
    
    # Analyze each session for suspicious content
    for session_id in sorted(sessions.keys(), reverse=True):
        session_data = sessions[session_id]
        print(f'\nüïê SESSION {session_id} ({len(session_data)} URLs):')
        
        suspicious = []
        legitimate = []
        
        for row in session_data:
            url = row[1] or ''
            title = row[2] or 'No title'
            scraped_at = row[3]
            
            # Check for the problematic URLs from the image
            if url == 'https://':
                suspicious.append(('EMPTY_HTTPS', url, title))
            elif 'copilot.microsoft.com' in url:
                suspicious.append(('COPILOT_MS', url, title))
            elif 'gnu.org/software/pspp' in url and ('(' in url or ')' in url):
                suspicious.append(('GNU_PSPP_MALFORMED', url, title))
            elif len(url) < 15 and url.startswith('http'):
                suspicious.append(('MALFORMED_SHORT', url, title))
            else:
                legitimate.append((url[:60] + '...', title[:30] + '...'))
        
        if suspicious:
            print(f'   üö® SUSPICIOUS URLs found ({len(suspicious)}):')
            for issue_type, url, title in suspicious:
                print(f'      {issue_type}: {url}')
                print(f'         Title: {title}')
        else:
            print(f'   ‚úÖ No suspicious URLs in this session')
        
        print(f'   üìä Legitimate URLs: {len(legitimate)}')
        if legitimate:
            print(f'      Sample: {legitimate[0][0]}')
    
    # Check session timeline
    print(f'\nüïê SESSION TIMELINE:')
    cur.execute('''
        SELECT session_id, MIN(scraped_at) as first_scrape, MAX(scraped_at) as last_scrape, COUNT(*) as url_count
        FROM scraped_contents 
        WHERE module_id = 1 
        GROUP BY session_id 
        ORDER BY session_id DESC
    ''')
    
    timeline = cur.fetchall()
    for session_id, first_scrape, last_scrape, url_count in timeline:
        print(f'   Session {session_id}: {url_count} URLs ({first_scrape} to {last_scrape})')
    
    conn.close()
    
except Exception as e:
    print(f'‚ùå Error: {e}')
    import traceback
    traceback.print_exc()
"
source venv/bin/activate
python3 -c "
import psycopg2

try:
    # Connect with correct credentials
    conn = psycopg2.connect(
        host='localhost',
        database='lmsguardian', 
        user='admin',
        password='group2'
    )
    
    cur = conn.cursor()
    
    print('üîç CHECKING DATABASE SCHEMA FIRST...')
    
    # Check the actual column names
    cur.execute('''
        SELECT column_name 
        FROM information_schema.columns 
        WHERE table_name = 'scraped_contents'
        ORDER BY ordinal_position
    ''')
    
    columns = [row[0] for row in cur.fetchall()]
    print(f'üìã Available columns: {columns}')
    
    # Now query with correct column names
    print(f'\n--- MODULE 1 ANALYSIS (with correct columns) ---')
    cur.execute('''
        SELECT session_id, url_link, title, scraped_at 
        FROM scraped_contents 
        WHERE module_id = 1 
        ORDER BY session_id DESC, scraped_at DESC
    ''')
    
    module1_data = cur.fetchall()
    print(f'üìä Total entries for Module 1: {len(module1_data)}')
    
    # Group by session
    sessions = {}
    for row in module1_data:
        session_id = row[0]
        if session_id not in sessions:
            sessions[session_id] = []
        sessions[session_id].append(row)
    
    print(f'üìã Sessions found for Module 1: {sorted(sessions.keys(), reverse=True)}')
    
    # Analyze each session for suspicious content
    for session_id in sorted(sessions.keys(), reverse=True):
        session_data = sessions[session_id]
        print(f'\nüïê SESSION {session_id} ({len(session_data)} URLs):')
        
        suspicious = []
        legitimate = []
        
        for row in session_data:
            url = row[1] or ''
            title = row[2] or 'No title'
            scraped_at = row[3]
            
            # Check for the problematic URLs from the image
            if url == 'https://':
                suspicious.append(('EMPTY_HTTPS', url, title))
            elif 'copilot.microsoft.com' in url:
                suspicious.append(('COPILOT_MS', url, title))
            elif 'gnu.org/software/pspp' in url and ('(' in url or ')' in url):
                suspicious.append(('GNU_PSPP_MALFORMED', url, title))
            elif len(url) < 15 and url.startswith('http'):
                suspicious.append(('MALFORMED_SHORT', url, title))
            else:
                legitimate.append((url[:60] + '...', title[:30] + '...'))
        
        if suspicious:
            print(f'   üö® SUSPICIOUS URLs found ({len(suspicious)}):')
            for issue_type, url, title in suspicious:
                print(f'      {issue_type}: {url}')
                print(f'         Title: {title}')
        else:
            print(f'   ‚úÖ No suspicious URLs in this session')
        
        print(f'   üìä Legitimate URLs: {len(legitimate)}')
        if legitimate:
            print(f'      Sample: {legitimate[0][0]}')
    
    # Check session timeline
    print(f'\nüïê SESSION TIMELINE:')
    cur.execute('''
        SELECT session_id, MIN(scraped_at) as first_scrape, MAX(scraped_at) as last_scrape, COUNT(*) as url_count
        FROM scraped_contents 
        WHERE module_id = 1 
        GROUP BY session_id 
        ORDER BY session_id DESC
    ''')
    
    timeline = cur.fetchall()
    for session_id, first_scrape, last_scrape, url_count in timeline:
        print(f'   Session {session_id}: {url_count} URLs ({first_scrape} to {last_scrape})')
    
    conn.close()
    
except Exception as e:
    print(f'‚ùå Error: {e}')
    import traceback
    traceback.print_exc()
"
python3 -c "
import psycopg2

try:
    conn = psycopg2.connect(
        host='localhost',
        database='lmsguardian', 
        user='admin',
        password='group2'
    )
    
    cur = conn.cursor()
    
    print('üîç INVESTIGATING MODULE 1 FOR STALE/CACHED DATA...')
    
    # Query Module 1 with correct column names
    cur.execute('''
        SELECT session_id, url_link, title, scraped_at, risk_category
        FROM scraped_contents 
        WHERE module_id = 1 
        ORDER BY session_id DESC, scraped_at DESC
    ''')
    
    module1_data = cur.fetchall()
    print(f'üìä Total entries for Module 1: {len(module1_data)}')
    
    if len(module1_data) == 0:
        print('‚ùå No data found for Module 1')
        conn.close()
        exit()
    
    # Group by session
    sessions = {}
    for row in module1_data:
        session_id = row[0]
        if session_id not in sessions:
            sessions[session_id] = []
        sessions[session_id].append(row)
    
    print(f'üìã Sessions found for Module 1: {sorted(sessions.keys(), reverse=True)}')
    
    # Analyze each session for the problematic URLs from the image
    total_suspicious = 0
    
    for session_id in sorted(sessions.keys(), reverse=True):
        session_data = sessions[session_id]
        print(f'\nüïê SESSION {session_id} ({len(session_data)} URLs):')
        
        suspicious = []
        legitimate = []
        
        for row in session_data:
            url = row[1] or ''
            title = row[2] or 'No title'
            scraped_at = row[3]
            risk_category = row[4] or 'None'
            
            # Check for the exact problematic URLs from the image
            if url == 'https://':
                suspicious.append(('EMPTY_HTTPS', url, title, scraped_at))
            elif 'copilot.microsoft.com' in url:
                suspicious.append(('COPILOT_MS', url, title, scraped_at))
            elif 'gnu.org/software/pspp' in url and ('(' in url or ')' in url):
                suspicious.append(('GNU_PSPP_MALFORMED', url, title, scraped_at))
            elif 'dlsweb.rmit.edu.au' in url and 'PG%20lit%20Review' in url:
                suspicious.append(('RMIT_PDF_ENCODED', url, title, scraped_at))
            elif len(url) < 15 and url.startswith('http'):
                suspicious.append(('MALFORMED_SHORT', url, title, scraped_at))
            else:
                legitimate.append((url, title, scraped_at))
        
        if suspicious:
            print(f'   üö® SUSPICIOUS URLs found ({len(suspicious)}):')
            total_suspicious += len(suspicious)
            for issue_type, url, title, scraped_at in suspicious:
                print(f'      {issue_type}: {url}')
                print(f'         Title: {title[:60]}...')
                print(f'         Scraped: {scraped_at}')
                print()
        else:
            print(f'   ‚úÖ No suspicious URLs in this session')
        
        print(f'   üìä Legitimate URLs: {len(legitimate)}')
        if legitimate:
            print(f'      Sample: {legitimate[0][0][:60]}...')
    
    print(f'\nüìä SUMMARY:')
    print(f'   Total suspicious URLs across all sessions: {total_suspicious}')
    print(f'   These are likely the source of the broken links in your report!')
    
    if total_suspicious > 0:
        print(f'\nüßπ RECOMMENDATION: Clean up the stale/cached data')
        print(f'   These URLs should not be part of legitimate course content')
    
    conn.close()
    
except Exception as e:
    print(f'‚ùå Error: {e}')
    import traceback
    traceback.print_exc()
"
PGPASSWORD=group2 psql -h localhost -U admin -d lmsguardian -c "
SELECT session_id, url_link, title, scraped_at 
FROM scraped_contents 
WHERE module_id = 1 
ORDER BY session_id DESC 
LIMIT 10;"
python3 -c "
import requests
import json

# Try to access the API directly
try:
    response = requests.get('http://127.0.0.1:8000/scrapedcontents/', timeout=5)
    print(f'API Status: {response.status_code}')
    
    if response.status_code == 200:
        data = response.json()
        print(f'Total records: {len(data)}')
        
        # Filter for Module 1 data
        module1_data = [item for item in data if item.get('module_id') == 1]
        print(f'Module 1 records: {len(module1_data)}')
        
        # Look for the problematic URLs from the image
        problematic_found = []
        
        for item in module1_data:
            url = item.get('url_link', '')
            title = item.get('title', 'No title')
            session_id = item.get('session_id')
            scraped_at = item.get('scraped_at', '')
            
            # Check for specific problematic patterns
            if url == 'https://':
                problematic_found.append(('EMPTY_HTTPS', url, title, session_id, scraped_at))
            elif 'copilot.microsoft.com' in url:
                problematic_found.append(('COPILOT_MS', url, title, session_id, scraped_at))
            elif 'gnu.org/software/pspp' in url and ('(' in url or ')' in url):
                problematic_found.append(('GNU_PSPP_MALFORMED', url, title, session_id, scraped_at))
            elif 'dlsweb.rmit.edu.au' in url and 'PG%20lit%20Review' in url:
                problematic_found.append(('RMIT_PDF_ENCODED', url, title, session_id, scraped_at))
        
        print(f'\\nüö® PROBLEMATIC URLs FOUND IN MODULE 1: {len(problematic_found)}')
        
        if problematic_found:
            print('\\nDETAILS:')
            for issue_type, url, title, session_id, scraped_at in problematic_found:
                print(f'   {issue_type}:')
                print(f'      URL: {url}')
                print(f'      Title: {title}')
                print(f'      Session: {session_id}')
                print(f'      Scraped: {scraped_at}')
                print()
            
            print('üí° CONCLUSION: These are stale/cached URLs that should not be in Module 1!')
            print('   They are likely from previous test runs or incorrect scraping.')
        else:
            print('‚úÖ No problematic URLs found in current Module 1 data')
            
            # Show sample of what IS in Module 1
            print('\\nSample of Module 1 URLs:')
            for item in module1_data[:5]:
                url = item.get('url_link', '')
                print(f'   {url[:60]}...')
    
    else:
        print(f'API Error: {response.status_code}')
        print(response.text)
        
except Exception as e:
    print(f'Error: {e}')
"
source venv/bin/activate && python -c "
import psycopg2

try:
    conn = psycopg2.connect(
        host='localhost',
        database='lmsguardian', 
        user='admin',
        password='group2'
    )
    
    cur = conn.cursor()
    
    print('ÔøΩÔøΩ CLEANING UP STALE/CONTAMINATED DATA FROM MODULE 1...')
    
    # Define the problematic URLs to remove
    problematic_urls = [
        'https://copilot.microsoft.com/',
        'https://',
        '(https://www.gnu.org/software/pspp/).', 
        'https://www.gnu.org/software/pspp/)'
    ]
    
    # Count how many we're about to remove
    print('üìä SCANNING FOR CONTAMINATED DATA:')
    total_found = 0
    for url in problematic_urls:
        cur.execute('''
            SELECT COUNT(*) FROM scraped_contents 
            WHERE module_id = 1 AND url_link = %s
        ''', (url,))
        count = cur.fetchone()[0]
        total_found += count
        print(f'   Found {count} instances of: {url}')
    
    print(f'\\nTotal contaminated records found: {total_found}')
    
    if total_found > 0:
        print('\\nüóëÔ∏è REMOVING CONTAMINATED DATA...')
        # Remove the contaminated data
        total_removed = 0
        for url in problematic_urls:
            cur.execute('''
                DELETE FROM scraped_contents 
                WHERE module_id = 1 AND url_link = %s
            ''', (url,))
            removed = cur.rowcount
            total_removed += removed
            if removed > 0:
                print(f'   ‚úÖ Removed {removed} instances of: {url}')
        
        # Commit the changes
        conn.commit()
        
        print(f'\\nüìä CLEANUP SUMMARY:')
        print(f'   Total contaminated records removed: {total_removed}')
    else:
        print('\\n‚úÖ No contaminated data found to remove')
    
    # Check what's left in Module 1
    cur.execute('''
        SELECT COUNT(*) FROM scraped_contents WHERE module_id = 1
    ''')
    remaining = cur.fetchone()[0]
    print(f'   Remaining legitimate records in Module 1: {remaining}')
    
    # Show sample of remaining data
    if remaining > 0:
        cur.execute('''
            SELECT url_link, title, session_id 
            FROM scraped_contents 
            WHERE module_id = 1 
            ORDER BY session_id DESC 
            LIMIT 5
        ''')
        
        samples = cur.fetchall()
        print(f'\\nüìã SAMPLE OF REMAINING MODULE 1 DATA:')
        for url, title, session_id in samples:
            print(f'   Session {session_id}: {url[:60]}...')
            print(f'      Title: {title[:40] if title else \"No title\"}...')
    
    conn.close()
    
    print(f'\\n‚úÖ CLEANUP COMPLETE!')
    print(f'   Your Module 1 reports should now be clean of contaminated data.')
    
except Exception as e:
    print(f'‚ùå Error during cleanup: {e}')
    import traceback
    traceback.print_exc()
"
python3 scraper/main.py
ls -la venv/
source venv/bin/activate && python -c "
import psycopg2

# --- Database Cleanup Script ---
# This script performs a targeted cleanup of specific, known-bad URLs
# from the scraped_contents table. It is designed to be safe and
# will not affect any legitimate data.

try:
    # Establish a connection to the database
    conn = psycopg2.connect(
        host='localhost',
        database='lmsguardian', 
        user='admin',
        password='group2'
    )
    cur = conn.cursor()
    
    print('üßπ Starting targeted database cleanup...')
    
    # Define a precise list of malformed URLs to be removed.
    # These are exact strings, ensuring no legitimate data is accidentally deleted.
    urls_to_delete = [
        'https://',
        'https://copilot.microsoft.com/',
        '(https://www.gnu.org/software/pspp/).'
    ]
    
    total_rows_deleted = 0
    
    # --- SAFETY SCAN FIRST ---
    print('\\nüìä Scanning for entries to be removed (no deletion yet)...')
    for url in urls_to_delete:
        cur.execute('SELECT COUNT(*) FROM scraped_contents WHERE url_link = %s', (url,))
        count = cur.fetchone()[0]
        if count > 0:
            print(f'   -> Found {count} entries for: \\'{url}\\'')
    
    # --- EXECUTE DELETION ---
    print('\\nüóëÔ∏è Deleting identified entries...')
    for url in urls_to_delete:
        # Execute the targeted DELETE command
        cur.execute('DELETE FROM scraped_contents WHERE url_link = %s', (url,))
        
        # Get the number of rows affected by the delete operation
        rows_deleted = cur.rowcount
        if rows_deleted > 0:
            print(f'   -> Deleted {rows_deleted} entries for: \\'{url}\\'')
            total_rows_deleted += rows_deleted
            
    if total_rows_deleted > 0:
        # Commit the transaction to make the changes permanent
        conn.commit()
        print(f'\\n‚úÖ Cleanup successful. Total records removed: {total_rows_deleted}')
    else:
        print('\\n‚úÖ No matching junk data found. Database is already clean.')

    # --- VERIFICATION ---
    print('\\nüîç Verifying that the junk data is gone...')
    for url in urls_to_delete:
        cur.execute('SELECT COUNT(*) FROM scraped_contents WHERE url_link = %s', (url,))
        count = cur.fetchone()[0]
        if count == 0:
            print(f'   -> Verified: 0 entries remain for \\'{url}\\'')
        else:
            print(f'   -> ‚ö†Ô∏è WARNING: {count} entries still exist for \\'{url}\\'')

    cur.close()
    conn.close()

except Exception as e:
    print(f'‚ùå An error occurred: {e}')

"
python3 scraper/main.py
sudo systemctl restart lmsguardian-uvicorn.service 
python3 scraper/main.py
sudo systemctl restart lmsguardian-uvicorn.service 
python3 scraper/main.py
ps aux | grep 'scraper/main.py'
python3 scraper/main.py
pip install -r requirements.txt
python3 scraper/main.py
pip install -r requirements.txt
python3 scraper/main.py
python3 scraper/reportgenerator/report.py
python3 scraper/reportgenerator/report.py | grep "Module 4"
python3 scraper/reportgenerator/report.py 2>&1 | grep -A 20 "Module 4"
python3 scraper/reportgenerator/report.py 2>&1 | grep -A 50 "Module 4"
source venv/bin/activate
python3 scraper/reportgenerator/report.py > report_output.log 2>&1 && tail -50 report_output.log
python3 scraper/reportgenerator/report.py
python3 scraper/reportgenerator/report.py 2>&1 | grep -A 10 "Module 4"
python3 scraper/reportgenerator/report.py
python3 cleanup_project.py
python3 scraper/main.py
find /home/administrator/Test-Chau-LMS/LMSTest -name "content_filter.py" -type f
git log --oneline -10
git show HEAD:content_filter.py | head -50
git show HEAD:content_filter.py > /tmp/complete_content_filter.py
git show HEAD:content_filter.py
python3 scraper/reportgenerator/report.py
cd /home/administrator/Test-Chau-LMS/LMSTest && python3 -c "
from content_filter.content_filter import is_paywall_url
url = 'https://kathyschwalbe.files.wordpress.com/2018/08/pmbok-guide-6th-edition-5-project-scope-management.pdf'
result, reason = is_paywall_url(url, '')
print(f'URL: {url}')
print(f'Is paywall: {result}')
print(f'Reason: {reason}')
"
python3 test_paywall_debug.py
cd /home/administrator/Test-Chau-LMS/LMSTest && python3 -c "
import psycopg2
from app.database.database import get_db
import os
from dotenv import load_dotenv

load_dotenv()

# Connect to database
conn = psycopg2.connect(
    host=os.getenv('DB_HOST'),
    database=os.getenv('DB_NAME'),
    user=os.getenv('DB_USER'),
    password=os.getenv('DB_PASSWORD'),
    port=os.getenv('DB_PORT')
)

cursor = conn.cursor()
query = '''
SELECT url, title, is_paywall, is_pornography, risk_category 
FROM scraped_content 
WHERE url LIKE '%kathyschwalbe.files.wordpress.com%'
LIMIT 5
'''

cursor.execute(query)
results = cursor.fetchall()

print('Database results for WordPress URL:')
for row in results:
    print(f'URL: {row[0][:80]}...')
    print(f'Title: {row[1]}')
    print(f'is_paywall: {row[2]}')
    print(f'is_pornography: {row[3]}')
    print(f'risk_category: {row[4]}')
    print('-' * 80)

cursor.close()
conn.close()
"
cd /home/administrator/Test-Chau-LMS/LMSTest && python3 -c "
import psycopg2
import os
from dotenv import load_dotenv

load_dotenv()

# Connect to database
conn = psycopg2.connect(
    host=os.getenv('DB_HOST'),
    database=os.getenv('DB_NAME'),
    user=os.getenv('DB_USER'),
    password=os.getenv('DB_PASSWORD'),
    port=os.getenv('DB_PORT')
)

cursor = conn.cursor()
query = '''
SELECT url, title, is_paywall, is_pornography, risk_category 
FROM scraped_content 
WHERE url LIKE '%kathyschwalbe.files.wordpress.com%'
LIMIT 5
'''

cursor.execute(query)
results = cursor.fetchall()

print('Database results for WordPress URL:')
for row in results:
    print(f'URL: {row[0][:80]}...')
    print(f'Title: {row[1]}')
    print(f'is_paywall: {row[2]}')
    print(f'is_pornography: {row[3]}')
    print(f'risk_category: {row[4]}')
    print('-' * 80)

cursor.close()
conn.close()
"
cd /home/administrator/Test-Chau-LMS/LMSTest && source venv/bin/activate && python3 -c "
import psycopg2
import os
from dotenv import load_dotenv

load_dotenv()

# Connect to database
conn = psycopg2.connect(
    host=os.getenv('DB_HOST'),
    database=os.getenv('DB_NAME'),
    user=os.getenv('DB_USER'),
    password=os.getenv('DB_PASSWORD'),
    port=os.getenv('DB_PORT')
)

cursor = conn.cursor()
query = '''
SELECT url, title, is_paywall, is_pornography, risk_category 
FROM scraped_content 
WHERE url LIKE '%kathyschwalbe.files.wordpress.com%'
LIMIT 5
'''

cursor.execute(query)
results = cursor.fetchall()

print('Database results for WordPress URL:')
for row in results:
    print(f'URL: {row[0][:80]}...')
    print(f'Title: {row[1]}')
    print(f'is_paywall: {row[2]}')
    print(f'is_pornography: {row[3]}')
    print(f'risk_category: {row[4]}')
    print('-' * 80)

cursor.close()
conn.close()
"
source venv/bin/activate && python3 check_wordpress_db.py
python3 check_wordpress_db.py
python3 debug_crawler_paywall.py
rm test_paywall_debug.py check_wordpress_db.py debug_crawler_paywall.py
python3 scraper/main.py
python3 -c "from content_filter import ContentFilter; print('‚úÖ ContentFilter import successful')"
python3 -c "from content_filter.content_filter import ContentFilter; print('‚úÖ Direct import successful')"
python3 scraper/main.py
python3 fix_wordpress_paywall.py
python3 scraper/reportgenerator/report.py
python3 scraper/main.py
cd /home/administrator/Test-Chau-LMS/LMSTest && python3 -c "
from content_filter import content_filter
url = 'https://kathyschwalbe.files.wordpress.com/2018/08/something.pdf'
is_paywall, reason = content_filter.is_paywall_url(url)
print(f'URL: {url}')
print(f'Is Paywall: {is_paywall}')
print(f'Reason: {reason}')
print()
print('Paywall domains:', content_filter.paywall_domains)
print('Paywall keywords:', content_filter.paywall_keywords)
"
cd /home/administrator/Test-Chau-LMS/LMSTest && python3 -c "
import sys
sys.path.insert(0, '/home/administrator/Test-Chau-LMS/LMSTest')
from content_filter.content_filter import ContentFilter

cf = ContentFilter()
url = 'https://kathyschwalbe.files.wordpress.com/2018/08/something.pdf'

print('Testing WordPress URL:', url)
print('Paywall keywords:', cf.paywall_keywords)
print()

# Test paywall detection step by step
from urllib.parse import urlparse
domain = urlparse(url).netloc.lower().replace('www.', '')
print('Extracted domain:', domain)
print('Known paywall domains:', cf.paywall_domains)
print('Domain in paywall list:', any(paywall_domain in domain for paywall_domain in cf.paywall_domains))
print()

# Test URL keywords
url_lower = url.lower()
print('URL lowercase:', url_lower)
print('Testing each paywall keyword:')
for keyword in cf.paywall_keywords:
    if keyword in url_lower:
        print(f'  ‚úÖ FOUND: \"{keyword}\" in URL')
    else:
        print(f'  ‚ùå Not found: \"{keyword}\"')
        
print()
is_paywall, reason = cf.is_paywall_url(url)
print(f'Final result: is_paywall={is_paywall}, reason={reason}')
"
cd /home/administrator/Test-Chau-LMS/LMSTest && python3 -c "
print('Testing WordPress paywall detection...')
url = 'https://kathyschwalbe.files.wordpress.com/2018/08/something.pdf'
paywall_keywords = ['paywall', 'subscription', 'premium', 'subscriber', 'sign up', 'login required', 'member', 'account required']

print('URL:', url)
print('URL lowercase:', url.lower())
print()

for keyword in paywall_keywords:
    if keyword in url.lower():
        print(f'FOUND KEYWORD: \"{keyword}\" in URL')
    else:
        print(f'Not found: \"{keyword}\"')
"
cd /home/administrator/Test-Chau-LMS/LMSTest && python3 -c "
import psycopg2
from urllib.parse import urlparse

# Connect to database
conn = psycopg2.connect(
    host='localhost',
    database='lms_guard',
    user='chau',
    password='123'
)
cursor = conn.cursor()

# Find the WordPress URL in recent entries
cursor.execute('''
    SELECT url_link, is_paywall, risk_category, created_at 
    FROM scraped_contents 
    WHERE url_link LIKE '%wordpress%' 
    ORDER BY created_at DESC 
    LIMIT 5
''')

results = cursor.fetchall()
print('Recent WordPress URLs in database:')
for url, is_paywall, risk_category, created_at in results:
    print(f'URL: {url}')
    print(f'is_paywall: {is_paywall}')
    print(f'risk_category: {risk_category}')
    print(f'created_at: {created_at}')
    print('-' * 50)

conn.close()
"
cd /home/administrator/Test-Chau-LMS/LMSTest && python3 -c "
import psycopg2

conn = psycopg2.connect(host='localhost', database='lms_guard', user='chau', password='123')
cursor = conn.cursor()

cursor.execute('''
    SELECT url_link, is_paywall, created_at 
    FROM scraped_contents 
    WHERE url_link LIKE '%wordpress%' 
    AND created_at >= '2025-07-19'
    ORDER BY created_at DESC
''')

results = cursor.fetchall()
print(f'WordPress URLs scraped today ({len(results)} found):')
for url, is_paywall, created_at in results:
    print(f'{created_at}: is_paywall={is_paywall} - {url[:80]}...')

conn.close()
"
python3 debug_wordpress.py
python3 test_paywall_detection.py
python3 test_html_paywall.py
python3 test_paywall_detection.py
python3 scraper/reportgenerator/report.py
python3 fix_wordpress_paywall.py
python3 scraper/main.py
python3 fix_wordpress_paywall.py
python3 test_scraper_detection.py
cd /home/administrator/Test-Chau-LMS/LMSTest && python3 -c "
import psycopg2
from datetime import datetime

conn = psycopg2.connect(host='localhost', database='lmsguardian', user='admin', password='group2')
cursor = conn.cursor()

print('Recent database entries:')
cursor.execute('''
    SELECT url_link, is_paywall, session_id, created_at 
    FROM scraped_contents 
    WHERE url_link LIKE '%wordpress%' 
    ORDER BY created_at DESC 
    LIMIT 5
''')

for url, is_paywall, session_id, created_at in cursor.fetchall():
    print(f'{created_at}: Session {session_id}, is_paywall={is_paywall}')
    print(f'  URL: {url[:80]}...')
    print()

conn.close()
"
cd /home/administrator/Test-Chau-LMS/LMSTest && python3 -c "
import psycopg2

try:
    conn = psycopg2.connect(host='localhost', database='lmsguardian', user='admin', password='group2')
    cursor = conn.cursor()

    print('üîç Checking latest database entries...')
    cursor.execute('''
        SELECT session_id, COUNT(*) as count, MAX(created_at) as latest
        FROM scraped_contents 
        GROUP BY session_id
        ORDER BY session_id DESC 
        LIMIT 5
    ''')

    results = cursor.fetchall()
    print(f'Found {len(results)} recent sessions:')
    for session_id, count, latest in results:
        print(f'  Session {session_id}: {count} entries, latest: {latest}')

    # Check specifically for WordPress URLs
    cursor.execute('''
        SELECT COUNT(*) 
        FROM scraped_contents 
        WHERE url_link LIKE '%wordpress%' 
    ''')
    
    wp_count = cursor.fetchone()[0]
    print(f'\\nTotal WordPress URLs in database: {wp_count}')

    conn.close()
except Exception as e:
    print(f'Database error: {e}')
"
find . -name "*.py" -path "*/paywall/*" -exec ls -la {} \;
find . -name "__pycache__" -type d -exec rm -rf {} + 2>/dev/null || true
cd /home/administrator/Test-Chau-LMS/LMSTest && python3 -c "
# Test the scraper detection with fresh imports
import sys
sys.path.insert(0, '/home/administrator/Test-Chau-LMS/LMSTest')

from scraper.scraper.crawler import detect_paywall_for_url

url = 'https://kathyschwalbe.files.wordpress.com/2018/08/something.pdf'
print(f'Testing: {url}')

# Test without HTML
result1 = detect_paywall_for_url(url)
print(f'Without HTML: {result1}')

# Test with minimal HTML (like a PDF page)
html = '<html><body>Download: <a href=\"file.pdf\">PDF</a></body></html>'
result2 = detect_paywall_for_url(url, html)
print(f'With HTML: {result2}')

# Test with very short content (this was triggering the bug)
short_html = '<html><body><p>PDF</p></body></html>'
result3 = detect_paywall_for_url(url, short_html)
print(f'With short HTML: {result3}')
"
find /home/administrator/Test-Chau-LMS/LMSTest -name "*.pyc" -delete && find /home/administrator/Test-Chau-LMS/LMSTest -name "__pycache__" -type d -exec rm -rf {} + 2>/dev/null || true
python3 fix_wordpress_paywall.py
python3 scraper/reportgenerator/report.py
python3 scraper/main.py
python3 fix_wordpress_paywall.py
python3 debug_all_detection.py
cd /home/administrator/Test-Chau-LMS/LMSTest && python3 -c "
from scraper.main import *
from scraper.scraper.crawler import detect_paywall_for_url

url = 'https://kathyschwalbe.files.wordpress.com/2018/08/something.pdf'
result = detect_paywall_for_url(url)
print(f'Scraper will use: detect_paywall_for_url(\"{url}\") = {result}')
"
cd /home/administrator/Test-Chau-LMS/LMSTest && python3 -c "
try:
    from scraper.scraper.crawler import run_crawler, detect_paywall_for_url
    print('‚úÖ Import successful - scraper can now use fixed detection logic')
    
    url = 'https://kathyschwalbe.files.wordpress.com/2018/08/something.pdf'
    result = detect_paywall_for_url(url)
    print(f'‚úÖ detect_paywall_for_url result: {result}')
except Exception as e:
    print(f'‚ùå Import failed: {e}')
"
python3 test_scraper_import.py
python3 fix_wordpress_paywall.py
python3 scraper/main.py
cd /home/administrator/Test-Chau-LMS/LMSTest && python3 -c "
try:
    from scraper.crawler import run_crawler, detect_paywall_for_url
    print('‚úÖ Import successful')
    
    url = 'https://kathyschwalbe.files.wordpress.com/2018/08/something.pdf'
    result = detect_paywall_for_url(url)
    print(f'‚úÖ Detection result: {result}')
except Exception as e:
    print(f'‚ùå Error: {e}')
"
python3 scraper/main.py
python3 fix_wordpress_paywall.py
cd /home/administrator/Test-Chau-LMS/LMSTest && python3 -c "
import sys
sys.path.insert(0, '/home/administrator/Test-Chau-LMS/LMSTest')

# Test what the scraper actually imports
from scraper.crawler import detect_paywall_for_url

url = 'https://kathyschwalbe.files.wordpress.com/2018/08/something.pdf'
print(f'Scraper imports detect_paywall_for_url: {detect_paywall_for_url}')
result = detect_paywall_for_url(url)
print(f'Result: {result}')
"
python3 debug_paywall_func.py
sudo systemctl restart lmsguardian-uvicorn.service 
python3 fix_wordpress_paywall.py 
python3 scraper/main.py
sudo systemctl status lmsguardian-uvicorn.service 
python3 scraper/main.py
ps aux | grep python
python3 scraper/main.py
cd /home/administrator/Test-Chau-LMS/LMSTest && python3 -c "
from scraper.scraper.crawler import detect_paywall_for_url
url = 'https://kathyschwalbe.files.wordpress.com/2018/08/project2016guide.pdf'
result = detect_paywall_for_url(url)
print(f'URL: {url}')
print(f'Paywall detected: {result}')
"
cd /home/administrator/Test-Chau-LMS/LMSTest && python3 -c "
from scraper.paywall.enhanced_detector import create_enhanced_detector
import asyncio

async def test_urls():
    detector = create_enhanced_detector()
    
    test_urls = [
        'https://kathyschwalbe.files.wordpress.com/2018/08/project2016guide.pdf',
        'https://kathyschwalbe.files.wordpress.com/2021/08/schwalbe-intro-pm-7e-sample-chapter-1.pdf',
        'https://www.wsj.com/finance/banking/goldman-sachs-greece-hotel-sell-34b5353a',
        'https://www.ft.com/content/2379dfdb-b62b-47de-98d5-dc49be50e661',
        'https://www.nytimes.com/subscription/all-access',
        'https://docs.python.org/3/',
        'https://en.wikipedia.org/wiki/Test'
    ]
    
    for url in test_urls:
        result = await detector.detect_paywall(url)
        status = 'üîí PAYWALL' if result['is_paywall'] else '‚úÖ CLEAN'
        print(f'{status} | {url}')
        print(f'    Confidence: {result[\"confidence\"]:.2f} | Methods: {result[\"detection_methods\"]}')
        if result['reasons']:
            print(f'    Reasons: {result[\"reasons\"]}')
        print()

asyncio.run(test_urls())
"
cd /home/administrator/Test-Chau-LMS/LMSTest && source venv/bin/activate && python -c "
from scraper.paywall.enhanced_detector import create_enhanced_detector
import asyncio

async def test_urls():
    detector = create_enhanced_detector()
    
    test_urls = [
        'https://kathyschwalbe.files.wordpress.com/2018/08/project2016guide.pdf',
        'https://kathyschwalbe.files.wordpress.com/2021/08/schwalbe-intro-pm-7e-sample-chapter-1.pdf',
        'https://www.wsj.com/finance/banking/goldman-sachs-greece-hotel-sell-34b5353a',
        'https://www.ft.com/content/2379dfdb-b62b-47de-98d5-dc49be50e661',
        'https://www.nytimes.com/subscription/all-access',
        'https://docs.python.org/3/',
        'https://en.wikipedia.org/wiki/Test'
    ]
    
    for url in test_urls:
        result = await detector.detect_paywall(url)
        status = 'üîí PAYWALL' if result['is_paywall'] else '‚úÖ CLEAN'
        print(f'{status} | {url}')
        print(f'    Confidence: {result[\"confidence\"]:.2f} | Methods: {result[\"detection_methods\"]}')
        if result['reasons']:
            print(f'    Reasons: {result[\"reasons\"]}')
        print()

asyncio.run(test_urls())
"
nmap -sV -p 80,443 --script http-headers wsj.com
sudo apt update && sudo apt install -y nmap
echo "=== Testing WSJ (Known Paywall) ===" && nmap -sV -p 80,443 --script http-headers,http-title wsj.com | head -20
nmap -p 80,443 --script http-title wsj.com
nmap -p 80,443 --script http-title wikipedia.org
nmap -p 443 --script http-headers,http-methods wsj.com
find . -name "*.pyc" -type f
python3 scraper/main.py
source venv/bin/activate
python3 scraper/main.py
python3 fix_wordpress_paywall.py 
source venv/bin/activate
python3 fix_wordpress_paywall.py 
python3 scraper/main.py
source venv/bin/activate
python3 scraper/main.py
cd ..
ls
cp -r LMSTest/ LMTest-backup-04-00-19-7
ls
cd LMTest-backup-04-00-19-7/
ls
cd ..
ks
ls
cd LMSTest/
source venv/bin/activate
python3 scraper/main.py
source venv/bin/activate && python3 scraper/main.py
python3 scraper/main.py
curl -s "http://127.0.0.1:8000/scrapedcontents/" | grep -A 5 -B 5 "coinlab"
curl -s "http://127.0.0.1:8000/scrapedcontents/" | python3 -m json.tool | grep -A 10 -B 2 "coinlab"
curl -s "http://127.0.0.1:8000/scrapedcontents/scan" | python3 -c "import sys, json; data=json.load(sys.stdin); [print(f'ID: {item[\"scraped_id\"]}, URL: {item[\"url_link\"]}, Location: {item.get(\"content_location\", \"No location\")}') for item in data if 'coinlab' in item['url_link']]"
git log --oneline -5
python3 scraper/main.py
cd ..
ls
cd LMSGuardianv2/
nano env
nano .env
cd scraper
cd scraper/
nano crawler.py 
cd ..
cd Test-Chau-LMS
ls
cd LMSTest
ls
python scraper/main.py > logtest.txt
source venv/bin/activate
python scraper/main.py > logtest.txt
[200~sudo systemctl restart lmsguardian-uvicorn.service
sudo systemctl restart lmsguardian-uvicorn.service
python scraper/main.py > logtest.txt
sudo systemctl restart lmsguardian-uvicorn.service
sudo journalctl -u lmsguardian-uvicorn.service -f
sudo systemctl restart lmsguardian-uvicorn.service
sudo journalctl -u lmsguardian-uvicorn.service -f
sudo systemctl restart lmsguardian-uvicorn.service
sudo journalctl -u lmsguardian-uvicorn.service -f
sudo systemctl restart lmsguardian-uvicorn.service
sudo journalctl -u lmsguardian-uvicorn.service -f
sudo systemctl restart lmsguardian-uvicorn.service
sudo journalctl -u lmsguardian-uvicorn.service -f
python scraper/main.py > logtest.txt
sudo systemctl restart lmsguardian-uvicorn.service
sudo journalctl -u lmsguardian-uvicorn.service -f
sudo systemctl restart lmsguardian-uvicorn.service
sudo journalctl -u lmsguardian-uvicorn.service -f
python scraper/main.py > logtest.txt
ls
cd LMSTest-State2/
source venv/bin/activate
python3 scraper/main.py
cd ..
ls
cp -r LMSTest/ LMSTest-State2
ls
cd LMSTest
ls
source venv/bin/activate
python3 scraper/main.py
python3 scraper/main.py
source venv/bin/activate && python3 scraper/main.py
ls
cd Test-Chau-LMS/
cd LMSTest-State2/
source venv/bin/activate
sudo systemctl status lmsguardian-uvicorn.service 
sudo nano /etc/systemd/system/lmsguardian-uvicorn.service
cd ..
cd LMSTest
nano .env
sudo journalctl -u lmsguardian-uvicorn.service -f
sudo systemctl restart lmsguardian-uvicorn.service
sudo journalctl -u lmsguardian-uvicorn.service -f
sudo systemctl restart lmsguardian-uvicorn.service
sudo journalctl -u lmsguardian-uvicorn.service -f
sudo systemctl restart lmsguardian-uvicorn.service
sudo journalctl -u lmsguardian-uvicorn.service -f
sudo systemctl restart lmsguardian-uvicorn.service
sudo journalctl -u lmsguardian-uvicorn.service -f
sudo systemctl restart lmsguardian-uvicorn.service
sudo journalctl -u lmsguardian-uvicorn.service -f
sudo systemctl restart lmsguardian-uvicorn.service
sudo journalctl -u lmsguardian-uvicorn.service -f
ps aux | grep uvicorn
cd /home/administrator/Test-Chau-LMS/LMSTest/
pkill -f uvicorn
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
ps aux | grep uvicorn
pkill -f uvicorn
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
ps aux | grep uvicorn
sudo systemctl restart lmsguardian-uvicorn.service
sudo journalctl -u lmsguardian-uvicorn.service -f
sudo systemctl restart lmsguardian-uvicorn.service
sudo journalctl -u lmsguardian-uvicorn.service -f
sudo systemctl restart lmsguardian-uvicorn.service
sudo journalctl -u lmsguardian-uvicorn.service -f
ls
cd Test-Chau-LMS
ls
cd LMSTest
cp -r lmstest lmstest1
ls
cd ..
cp -r lmstest lmstest1
cp -r LMStest lmstest1
cp -r LMSTest lmstest1
ls
cd lmstest1
source venv/bin/activate
python scraper/main >logtest.txt
ls
python scraper/main.py > logtest.txt
ls
python scraper/main.py > logtest.txt
python scraper/main.py > /home/administrator/Test-Chau-LMS/lmstest1/scraper/logtest.txt 2>&1
grep -rn "Paywall detection not available" /home/administrator/Test-Chau-LMS/LMSTest/
python scraper/main.py > /home/administrator/Test-Chau-LMS/lmstest1/scraper/logtest.txt 2>&1
cat .env
python scraper/main.py > /home/administrator/Test-Chau-LMS/lmstest1/scraper/logtest.txt 2>&1
python scraper/scraper/savelocal.py
python scraper/reportgenerator/report.py
python scraper/scraper/savelocal.py
python scraper/reportgenerator/report.py
cat .env
python scraper/reportgenerator/report.py
[200~sudo rm -rf /var/www/html/localrepo/*~
sudo rm -rf /var/www/html/localrepo/*
cd Test-Chau-LMS
ls
cd lmstest1
ls
source venv/bin/activate
ls
python scraper/main.py >logtest.txt
[200~python3 main/scraper.py > scraper/logtest.txt 2>&1
python3 main/scraper.py > scraper/logtest.txt 2>&1
python3 /home/administrator/Test-Chau-LMS/lmstest1/scraper/main.py > /home/administrator/Test-Chau-LMS/lmstest1/scraper/logtest.txt 2>&1
cd scraper
ls
cd scraper
ls
python savelocal.py
cd ..
cd. ..
python scraper/main.py
cd ..
python scraper/main.py
python scraper/reportgenerator/report.py > logtest.txt
python scraper/reportgenerator/report.py
python scraper/main.py
python scraper/reputation/checker.py
python scraper/main.py
python scraper/reputation/checker.py
python scraper/main.py
ls
cd Test-Chau-LMS
cd
ls
cd Test-Chau-LMS
LS
ls
cd lmstest1
nano .env
LS
ls
cd ..
cd lmstest1
ls
python scraper/reportgenerator/report.py
source venv/bin/activate
python scraper/reportgenerator/report.py
python scraper/main.py
source venv/bin/activate
python scraper/main.py
cd Test-Chau-LMS
ls
cd lmstest1
ls
cd scraper
ls
cd reportgeneratory
cd reportgenerator
source venv/bin/activate
cd ..
source venv/bin/activate
python scraper/reportgenerator/report.py
python scraper/main.py
pip install openpyxl
python scraper/main.py
sudo apt update
sudo apt install binutils
python scraper/main.py
ls
source venv/bin/activate
ls
python scraper/main.py
ls
cd ..
ls
cd lmstest1
ls
source venv/bin/activate
python scraper/main.py
ls
cd ..
ls
cp -r lmstest1 lmstest2
ls
cd Test-Chau-LMS/
ls
cd lmsguardianv1/
ls
source venv/bin/activate
python scraper/main.py
ls
cd Test-Chau-LMS/
ls
cp -r lmstest1 ./LMSFinal
ls
cd ..
ls
cd Test-Chau-LMS/
ls
cp -r LMSFinal/ ..
ls
cd ..
ls
sudo systemctl status lmsguardian-uvicorn.service
sudo nano /etc/systemd/system/lmsguardian-uvicorn.service
ls
sudo systemctl daemon-reload
sudo nano /etc/systemd/system/lmsguardian-uvicorn.service
sudo systemctl status lmsguardian-uvicorn.service
sudo systemctl restart lmsguardian-uvicorn.service
sudo systemctl status lmsguardian-uvicorn.service
sudo systemctl stop lmsguardian-uvicorn.service
sudo systemctl status lmsguardian-uvicorn.service
sudo systemctl daemon-reload
sudo systemctl status lmsguardian-uvicorn.service
sudo nano /etc/systemd/system/lmsguardian-uvicorn.service
sudo systemctl start lmsguardian-uvicorn.service
sudo systemctl status lmsguardian-uvicorn.service
sudo systemctl stop lmsguardian-uvicorn.service
kill -p 609117
kill 609117
sudo lsof -i :8000
sudo systemctl stop lmsguardian-uvicorn.service
sudo systemctl start lmsguardian-uvicorn.service
sudo systemctl status lmsguardian-uvicorn.service
sudo nano /etc/systemd/system/lmsguardian-uvicorn.service
sudo nano /system.slice/lmsguardian-uvicorn.service
sudo fuser 8000/tcp
sudo kill 609155
sudo nano /system.slice/lmsguardian-uvicorn.service
sudo nano /etc/systemd/system/lmsguardian-uvicorn.service
sudo systemctl stop lmsguardian-uvicorn.service
sudo fuser 8000/tcp
sudo systemctl daemon-reload
sudo systemctl start lmsguardian-uvicorn.service
sudo systemctl status lmsguardian-uvicorn.service
ip a
sudo reboot
psql -U admin lmsguardian
psql -U admin -d lmsguardian

psql -U admin -d lmsguardians
psql -U admin -d lmsguardian
cd Test-Chau-LMS
ls
cd lmstest1
psql -U admin -d lmsguardian -f gwans.sql
python scraper/main.py
source venv/bin/activate
ls
python scraper/main.py
cat .env
python scraper/main.py
sudo systemctl status lmsguardian-uvicorn.service 
sudo nano /etc/systemd/system/lmsguardian-uvicorn.service
sudo systemctl start lmsguardian-uvicorn.service 
sudo systemctl status lmsguardian-uvicorn.service 
sudo systemctl enable lmsguardian-uvicorn.service 
ls
cd LMSFinal/
ls
source venv/bin/activate
python scraper/main.py
ls
date --utc
timedatectl
date --utc
timedatectl
sudo systemctl status n8n
sudo nano /etc/systemd/system/n8n.service
sudo systemctl daemon-reload
sudo systemctl restart n8n
LS
ls
cd LMSFinal
nano .env
sudo systemctl status lmsguardian-uvicorn.service 
sudo systemctl status n8n.service
clear
ls
cd LMSFinal/
ls
source venv/bin/activate
python scraper/main.py
ls
cd LMSFinal
ls
source venv/bin/activate
python scraper/main.py
cat .env
python scraper/main.py
psql -U admin \d lmsguardians -f gwans.sql
psql -U admin -d lmsguardians -f gwans.sql
psql -U admin -d lmsguardian -f gwans.sql
python scraper/main.py
ls
cd LMSFinal
ls
source venv/bin/activate
rm -rf /home/administrator/LMSFinal/localrepo
sudo rm -rf /home/administrator/LMSFinal/localrepo
python scraper/main.py
clamscan eicar.txt
echo 'X5O\!P%@AP[4\PZX54(P^)7CC)7}$EICAR-STANDARD-ANTIVIRUS-TEST-FILE!$H+H*' > eicar.txt
clamscan eicar.txt
nano /home/administrator/LMSFinal/eicar.txt
cat eicar.txt
clamscan eicar.txt
clamscan eicar_test.docx
ls
clamscan eicar_test.docx
ls
clamscan seicar_test.docx
zip eicar_test.zip seicar_test.docx
libreoffice --headless --convert-to doc seicar_test.docx
echo "id,name,comment

echo 'id,name,comment
1,virus,X5O\!P%@AP[4\PZX54(P^)7CC)7}$EICAR-STANDARD-ANTIVIRUS-TEST-FILE!$H+H*' > eicar_test.csv
clamscan eicar_test.csv
nano eicar_test.csv
clamscan eicar_test.csv
sudo freshclam
clamscan eicar_test.csv
file eicar_test.csv
cat eicar_test.csv
clamscan X50.doc
ls
clamscan X5O.doc
cat x5o.doc
cat X5O.doc
PuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTYPuTTY
nano X5O.doc
python scraper/main.py
LS
ls
sudo cp -r LMSFinal LMSGuardianv3
ls
git remote -v
git status
git add .
git commit -m "Update LMSGuardianG2: add, modify, and delete files as of 2025-07-26"
git push origin main
source venv/bin/activate
python scraper/main.py
cd LMSFinal
ls
source venv/bin/activate
psql -U admin -d lmsguardians -f gwans.sql
psql -U admin -d lmsguardian -f gwans.sql
python scraper/main.py
psql -U admin -d lmsguardians -f gwans.sql
psql -U admin -d lmsguardian -f gwans.sql
\q
psql -U admin -d lmsguardian -f gwans.sql
python scraper/main.py
psql -U admin -d lmsguardian -f gwans.sql
psql -U admin -d lmsguardian 
systemctl list-units --type=service --state=running
ps aux | grep uvicorn
uvicorn app.main:app --host 0.0.0.0 --port 8000
sudo lsof -i :8000
ps aux | grep uvicorn
sudo kill 1451
ps aux | grep uvicorn
systemctl list-units --type=service | grep uvicorn
sudo systemctl stop lmsguardian-uvicorn.service
systemctl list-units --type=service | grep uvicorn
sudo systemctl restart lmsguardian-uvicorn.service
sudo journalctl -u lmsguardian-uvicorn.service -f
systemctl list-units --type=service | grep uvicorn
sudo systemctl restart lmsguardian-uvicorn.service
systemctl list-units --type=service | grep uvicorn
sudo journalctl -u lmsguardian-uvicorn.service -f
python scraper/main.py
ps aux | grep uvicorn
sudo nano /etc/systemd/system/lmsguardian-uvicorn.service
sudo systemctl restart lmsguardian-uvicorn.service
sudo journalctl -u lmsguardian-uvicorn.service -f
sudo systemctl daemon-reload
sudo systemctl restart lmsguardian-uvicorn.service
sudo journalctl -u lmsguardian-uvicorn.service -f
ps aux | grep uvicorn
sudo nano /etc/systemd/system/lmsguardian-uvicorn.service
sudo systemctl daemon-reload
sudo systemctl restart lmsguardian-uvicorn.service
sudo journalctl -u lmsguardian-uvicorn.service -f
rm -rf /home/administrator/Test-Chau-LMS/LMSTest/venv
ps aux | grep uvicorn
sudo systemctl daemon-reexec
sudo systemctl daemon-reload
sudo systemctl restart lmsguardian-uvicorn.service
ps aux | grep uvicorn
sudo systemctl restart lmsguardian-uvicorn.service
sudo nano /etc/systemd/system/lmsguardian-uvicorn.service
sudo systemctl daemon-reexec
sudo systemctl daemon-reload
sudo systemctl restart lmsguardian-uvicorn.service
sudo systemctl status lmsguardian-uvicorn.service
ls -l /home/administrator/LMSFinal/venv/bin/uvicorn
head -n 1 /home/administrator/LMSFinal/venv/bin/uvicorn
pip uninstall uvicorn -y
pip install uvicorn
pip uninstall uvicorn -y --break-system-packages
pip install uvicorn --break-system-packages
head -n 1 venv/bin/uvicorn
pip uninstall uvicorn -y --break-system-packages
pip install uvicorn --break-system-packages
head -n 1 venv/bin/uvicorn
pip freeze | grep -v '^uvicorn' > requirements.txt
deactivate
rm -rf venv
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
pip install uvicorn
head -n 1 venv/bin/uvicorn
sudo systemctl daemon-reload
sudo systemctl restart lmsguardian-uvicorn.service
sudo systemctl status lmsguardian-uvicorn.service
sudo journalctl -u lmsguardian-uvicorn.service -f
pip install fastapi
sudo journalctl -u lmsguardian-uvicorn.service -f
sudo systemctl restart lmsguardian-uvicorn.service
sudo journalctl -u lmsguardian-uvicorn.service -f
pip install sqlalchemy
sudo journalctl -u lmsguardian-uvicorn.service -f
sudo systemctl restart lmsguardian-uvicorn.service
sudo journalctl -u lmsguardian-uvicorn.service -fdd
pip install python-dotenv
sudo systemctl restart lmsguardian-uvicorn.service
sudo journalctl -u lmsguardian-uvicorn.service -f
pip install psycopg2-binary
sudo systemctl restart lmsguardian-uvicorn.service
sudo journalctl -u lmsguardian-uvicorn.service -f
psql -U admin -d lmsguardian -f gwans.sql
python scraper/main.py

psql -U admin -d lmsguardian -f gwans.sql
python scraper/main.py
pip install httpx
python scraper/main.py
pip install playwright
python scraper/main.py
pip install pytz
python scraper/main.py
pip install python-docx
python scraper/main.py
pip install oletools
python scraper/main.py
pip install pymupdf
python scraper/main.py
pip install python-pptx
python scraper/main.py
pip install openpyxl
python scraper/main.py
pip install beautifulsoup4
python scraper/main.py
playwright install
python scraper/main.py
sudo systemctl restart lmsguardian-uvicorn.service
sudo journalctl -u lmsguardian-uvicorn.service -f
python scraper/main.py
psql -U admin -d lmsguardian -f gwans.sql
python scraper/main.py
cd LMSFinal
ls
source venv/bin/activate
sudo journalctl -u lmsguardian-uvicorn.service 
clear
ls
python scraper/main.py
ls
tar -czvf LMSFinal_backup_$(date +%Y%m%d).tar.gz LMSFinal/
cd LMSFinal
ls
source venv/bin/activate
pip freeze > requirements.txt
psql -U admin -d lmsguardian
python scraper/main.py
python /home/administrator/LMSFinal/scraper/downloadlocalAPA7/main.py
python scraper/main.py
python scraper/main.pydf -h
df -h
find ~ -type f \( -name "*.zip" -o -name "*.tar.gz" -o -name "*.tar" \)
tar -czvf LMSFinal_backup_20250730.tar.gz LMSFinal/
ls -lh LMSFinal_backup_*.tar.gz
mkdir -p ~/steve
tar -xzvf LMSFinal_backup_20250730.tar.gz -C ~/steve
ls ~/steve
python scraper/main.pydf -h
python scraper/main.py
touch scraper/__init__.py
touch scraper/scraper/__init__.py
touch scraper/paywall/__init__.py
python scraper/main.py
sudo systemctl status lmsguardian-uvicorn.service 
lsb_release -a
cat /etc/os-release
apt show mariadb-server
clear
apt show mariadb-server
pip show fastapi
ls
cd LMSFinal/
ls
source venv/bin/activate
pip show fastapi
nano requirements.txt 
ls
tar -czvf LMSFinal_backup_$(date +%Y%m%d).tar.gz LMSFinal/
ls
cd LMSFinal
source venv/bin/activate
python scraper/main.py
cat .env
nano .env
python scraper/reputation/checker.py
cat .env
python scraper/reputation/checker.py
cat .env
python scraper/reputation/checker.py
cat .env
python scraper/reputation/checker.py
python scraper/main.py
cd LMSFinal
source venv/bin/activate
python scraper/main.py
cat .env
nano .env
cat .env
nano .env
python scraper/main.py
ls
clear
cd LMSFinal/
source venv/bin/activate
python scraper/main.py
clear
python scraper/main.py
cd LMSFinal
source venv/bin/activate
sudo apt update
sudo apt install graphviz
pyan3 scraper/scraper/crawler.py --uses --no-defines --colored --dot > callgraph.dot
pip install pyan3
pyan3 scraper/scraper/crawler.py --uses --no-defines --colored --dot > callgraph.dot
pip uninstall pyan3
pip install pyan3==0.9.5
nano ~/LMSFinal/venv/lib/python3.11/site-packages/pyan/main.py
pip install pyan3
nano ~/LMSFinal/venv/lib/python3.11/site-packages/pyan/main.py
pyan3 scraper/scraper/crawler.py --uses --no-defines --colored --dot > callgraph.dot
dot -Tpng callgraph.dot -o callgraph.png
pyan3 scraper/scraper/*.py --uses --colored --dotfile callgraph.dot
touch scraper/__init__.py
touch scraper/scraper/__init__.py
pyan3 scraper/scraper/crawler.py scraper/scraper/utils.py scraper/scraper/downloadfiles.py --uses --colored --dotfile callgraph.dot
python content_filter/content_filter.py
cat .env
python content_filter/content_filter.py
python scraper/main.py
python content_filter/content_filter.py
ls
cd LMSFinal/
source venv/bin/activate
python scraper/main.py
cd LMSFinal
source venv/bin/activate
python scraper/main.py
cd /home/administrator/LMSFinal && source venv/bin/activate && python scraper/main.py
ls
cd ..
ls -l LMSFinal
ls /home/administrator/.cache/ms-playwright/
clear
python scraper/main.py
cd LMSFinal/
python scraper/main.py
clear
psql -U admin -d lmsguardian
clear
psql -U admin -d lmsguardian
clear
python scraper/main.py
sudo systemctl status lmsguardian-uvicorn.service 
sudo systemctl status n8n.service 
